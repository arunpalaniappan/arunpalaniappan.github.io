<html>

<head>
	<script async src="https://www.googletagmanager.com/gtag/js?id=UA-168258233-1"></script>
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());
		gtag('config', 'UA-168258233-1');
	</script>
	<title>Stochastic Bandits</title>
	<link rel = "stylesheet" type = "text/css" href = "../post.css" />
	<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
	<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3.0.1/es5/tex-mml-chtml.js"> </script>
</head>

<body>
<section>
	<div class="post">
		<h1><center>Stochastic Bandits</center></h1>
		<h2> Introduction </h2>
		In sequential decision making where the objective is to choose the optimal decision, a judicious choice of sequential plan can bring about a considerable reduction in the average sample size necessary to find the optimal decision. Consider a scenario where there are two different treatments available for a disease out of which one treatment is superior than the other. We can choose a fixed sample size and do the tests in that sample and find which treatment is better. But by a judicious choice of a sequential plan, we can bring about a reduction in the sample size required to find which treatment is better. When we feel pretty sure that one treatment is superior to the other, we can end the experiment. This will result in less number of people getting inferior treatment. Bandit algorithms are a class of algorithms which deals with sequential decision making problems.

		<h2> Stochastic Bandit </h2>
		We consider a bandit model where the rewards for the arms are independent and identically distributed. Such a model is called as a stochastic bandit model. There are \(K \) possible actions (arms) to choose from in each round. During the \(n \) rounds, the algorithm chooses a arm and collects the corresponding rewards. The interaction protocol is as follows:
		<div class="algorithm">
			<b>Given:</b> K arms, n rounds <br>
				In rount \(t \):
				<ol>
					<li> Algorithm pick'a arm \(a_{t} \) from the action set \( A = 1, 2, ... ,k \)  according to a policy \( \pi \) </li>
					<li> Algorithm observes reward \( X_{t} \) for the chosen arm. </li>
				</ol>
		</div>
		In a stochastic bandit model, 
		<ul>
			<li> The reward for each arm follows <i>a</i> follows a probability distribution \( P_{a} \) with mean \( \mu_{a} \). </li>
			<li>The rewards for the arms are independent and identically distributed. </li>
		</ul>
		The policy is a mapping from histories to actions. It says what action to choose at time \( t \) based on the history of past action and rewards upto time \( t - 1\). If the reward \( X_{t} \in {0, 1} \) is binary-values and probability of recieving reward \( X_{t} = 1 \) by picking arm <i>a</i> is \( \mu_{a} \), such a bandit-model is called as stochastic-bernoulli bandit.
			
		<div class="note">
			<b>Note:</b>
			The point of bandit models is to predict the performance of algorithms on future problem instances.  <br>
				-Lattimore T, Szepesv√°ri C. <a href="https://tor-lattimore.com/downloads/book/book.pdf">Bandit algorithms</a>, 2018.			
		</div>
			
		<h3>Regret</h3>
		How do we measure the performance of bandit algorithms? An algorithm is said to be efficient if it maximizes the expected cumulative reward. In case of a stochastic bernoulli bandit, to maximize the cumulative reward, we have to choose the arm which has the largest mean of all arms. The performance of the bandit algorithms can be measured in terms of how much our collected reward has deviated from the maximum possible reward. This gives us the following definition of regret: $$ R(n) = n. \mu^{*} - \mathbb{E}[\sum_{t=1}^{n} X_t]$$ where \( \mu^{*} =  \text{max}_{a \in A}  \mu_a \). Minimizing regret is equivalent to maximizing reward collected by the learner.
			
		<h3>Policy</h3>
		The policy \( \pi \) is a mapping from histories to actions. Based on the past actions and rewards, the policy dictates the action to be taken by the learner in the next round. There are different policies like explore-then-commit, upper confidence bounds. The learner adopts a policy to interact with the environment. A good policy has a low regret.
		
		<h3>Environment Class</h3>
		A bandit problem is a sequential game between a learner and an environment. The game is played over \( n \) rounds and \( n \) as horizon. In the game, the learner chooses action and the environment gives reward to learner for the chosen action. The only thing that learner knows is that true environment ( the type of bandit problem ) lies in some set \( e \). A policy which has a low regret in one environment may have a high regret in some other environment. An appropraite policy has to be selected for use in an environment.
	
		The environment may follow a bernoulli reward distribution with an unknown mean vector for the arms or the environment's reward distribution may also follow a normal distribution with an unknown mean vector and variance vector. There is the adversarial bandit scenario where the the environment deliberately plans rewards so as to maximize the regret and there are policies to address the adversarial bandit scenario.
		
		<!-- Frame the above with a better choice of rewards-->
		
		<h2>Applications of stochastic bandit</h2>
		<ul>
			<li>When a user visits a webpage, a learning algorithm selects one of the many possible ads to display. The differetn ads are different arms of the bandit and when ad \( a \) is displayed, the learning algorithm observes whether a user clicks the ad or not. When the user clicks the ad, the algorithm receives a reward \( X_t = 1\), otherwise 0. </li>
			<li> Suppose you are a supply chain manager in automobile industry and you have to source a automobile component. Five different manufacturers provide the same component. But there are defective goods in the supply. You could use a bandit algorithm to find the best manufacturer at the shortest possible time. The five different suppliers will be the five different arms of bandits and the reward \( X_t \) can be defined as the number of non-defective goods in a sample of 100 goods. </li>
		</ul>
			
		<h2>Bibliographical remarks and further directions</h2>
		<a href="https://www.ams.org/journals/bull/1952-58-05/S0002-9904-1952-09620-8/S0002-9904-1952-09620-8.pdf">Robbins 1952</a> provide a good read on sequential design of experiments. This <a href="https://banditalgs.com/2016/09/04/stochastic-bandits-warm-up/">blog</a> gives a more detailed read on stochastic bandit.
	</div>
</section>
</body>
</html>

