<html>

<head>
	<script async src="https://www.googletagmanager.com/gtag/js?id=UA-168258233-1"></script>
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());
		gtag('config', 'UA-168258233-1');
	</script>
	<title>Multi Armed Bandits</title>
	<link rel = "stylesheet" type = "text/css" href = "../post.css" />
</head>

<body>
<section>
	<div class="post">
		<h1><center> Multi Armed Bandits </center></h1>
		<h1> Introduction </h1>
		Decision making under uncertatinity is difficult. The difficulty arises due to exploration-exploitation dilemma. When a decision maker 	has to make a decision,  it has got two choices.
		<ol>
			<li> Choose the best decision based on past experience (exploitation of past knowledge).
			<li>  Choose a decision whose outcome it is uncertain about (exploration).
		</ol>
		This kind of problem is faced many a times in day to day life. Consider going to a restaurent. You can choose the best restaurent which you know if you exploit your knowledge. Doing so, you are guaranteed a minimum satisifaction.  But there might be a new restaurent where you have not been and it can prove to to be even better or it can turn out to be worse. You take the risk of exploring to gain knowledge when you go to a new restaurent. Bandit problems are the class of problem which solves the problem of how well a decision maker could manage the exploration-exploitation trade off.
		<h1> The Multi-Armed Bandit Problem </h1>
		A multi arm bandit problem is a sequential decision making problem where the learning agent has to choose different actions. Actions are often called as arms in the literature. After an arm is pulled, the agent receives a reward.  The reward recieved by pulling an arm follows a probability distribution with unknown mean. The reward of the arms are stochastic and the learning agent does not know the underlying probability distributions which generated the reward. The objective for the agent is to maximize itâ€™s expected cumulativereward. Since the agent does not know the mean rewards of the arm in advance, the agent faces the exploration-exploitation dilemma. At each step, the agent can pull a random arm - explore and learn more about the arm or exploit, and pull the arm with the highest average reward so far.

		<h1> Applications </h1>
		Bandit Algorithms can be used to model sequential-decision making problems.  Here, I give a flavour of very few of the applications of bandit algorithms.

		<h2> Healthcare </h2>
		Clinical trials are conducted to choose between two drugs for a treatment. This problem can be framed as a bandit  problem by modelling each drug as a arm which is to be choosen as treatment for each patient and the reward can be modelled as the response ofthe patient for the drug.  The objective for the physician is to come to a quick conclusion on which drug is better for the treatment with a statistical significance.

		<h2>Recommender Systems</h2>
		Consider the problem of maximizing the effectiveness of product recommendations. A user who is interested in the recommended product will click on it. There will be many products to recommend. The different products will be the different arms of the bandit. The  objective  of  the  learner  is  to  learn  the  preferences  of  the  user  and  maximize  the effectiveness of recommendations shown.

		<h2>Dynamic Prcing</h2>
		In dynamic pricing, a company wants to choose the price of the product which results inhigh yield for the company.  Yield for the company can be a combination of revenue and profit on selling the product at that price. The different prices will be different actionsfor the learner and the objective for the learner is to choose the price which maximizesthe revenue.

		<h1>Further Resources</h1>
		<a href="https://www.youtube.com/watch?v=AizR8uvhX-s">Bandit Optimilaties lecture by Prof. Balaraman Ravindran</a>
	</div>
	
	<div class="space">
		
	</div>
</section>
</body>
</html>
